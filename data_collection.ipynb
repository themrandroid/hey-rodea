{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2470a50f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset, Audio\n",
    "import pandas as pd\n",
    "import os, io\n",
    "import soundfile as sf\n",
    "from pydub import AudioSegment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d43e7832",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File already exists.\n"
     ]
    }
   ],
   "source": [
    "file_path = \"metadata_commonvoice.csv\"\n",
    "\n",
    "# Check if the metadata already exists\n",
    "if os.path.isfile(file_path):\n",
    "    print(\"File already exists.\")\n",
    "else:\n",
    "    # Paths\n",
    "    cv_root = \"cv-corpus-22.0-delta-2025-06-20/en\"\n",
    "    clips_dir = os.path.join(cv_root, \"clips\")\n",
    "    validated_tsv = os.path.join(cv_root, \"validated.tsv\")\n",
    "\n",
    "    # Load validated metadata\n",
    "    validated = pd.read_csv(validated_tsv, sep=\"\\t\")\n",
    "\n",
    "    rows = []\n",
    "    for _, row in validated.iterrows():\n",
    "        mp3_file = os.path.join(clips_dir, row[\"path\"])\n",
    "        wav_file = mp3_file.replace(\".mp3\", \".wav\")\n",
    "\n",
    "        # Convert MP3 â†’ WAV if not already done\n",
    "        if not os.path.exists(wav_file):\n",
    "            audio = AudioSegment.from_mp3(mp3_file)\n",
    "            audio = audio.set_frame_rate(16000).set_channels(1)\n",
    "            audio.export(wav_file, format=\"wav\")\n",
    "\n",
    "        # Extract duration + sampling rate\n",
    "        with sf.SoundFile(wav_file) as f:\n",
    "            duration = len(f) / f.samplerate\n",
    "            rows.append({\n",
    "                \"file\": wav_file,\n",
    "                \"duration_sec\": round(duration, 2),\n",
    "                \"sampling_rate\": f.samplerate,\n",
    "                \"dataset\": \"CommonVoice\",\n",
    "                \"speaker_id\": row.get(\"client_id\", \"\"),\n",
    "                \"environment\": \"unknown\",\n",
    "                \"text\": row.get(\"sentence\", \"\")\n",
    "            })\n",
    "\n",
    "    cv_df = pd.DataFrame(rows)\n",
    "    cv_df.to_csv(\"metadata_commonvoice.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c46fea7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File already exists.\n"
     ]
    }
   ],
   "source": [
    "file_path = \"librispeech_metadata.csv\"\n",
    "\n",
    "# Check if the metadata already exists\n",
    "if os.path.isfile(file_path):\n",
    "    print(\"File already exists.\")\n",
    "else:\n",
    "    # 1. Load dataset without decoding (skip torchcodec)\n",
    "    dataset = load_dataset(\"librispeech_asr\", \"clean\")\n",
    "    dataset = dataset.cast_column(\"audio\", Audio(decode=False))\n",
    "\n",
    "    # 2. Take a small subset for testing\n",
    "    subset = dataset[\"train.100\"].shuffle(seed=42).select(range(200))\n",
    "\n",
    "    # 3. Output folder\n",
    "    out_dir = \"librispeech_subset_wav\"\n",
    "    os.makedirs(out_dir, exist_ok=True)\n",
    "\n",
    "    rows = []\n",
    "    for i, item in enumerate(subset):\n",
    "        audio_bytes = item[\"audio\"][\"bytes\"]   # raw FLAC bytes\n",
    "        speaker_id = item[\"speaker_id\"]\n",
    "        text = item.get(\"text\", \"\")\n",
    "\n",
    "        # Decode bytes with soundfile\n",
    "        audio_array, sr = sf.read(io.BytesIO(audio_bytes))\n",
    "        duration_sec = len(audio_array) / sr\n",
    "\n",
    "        # Save as WAV\n",
    "        out_path = os.path.join(out_dir, f\"sample_{i}.wav\")\n",
    "        sf.write(out_path, audio_array, sr)\n",
    "\n",
    "        # Metadata row\n",
    "        rows.append({\n",
    "            \"file\": out_path,\n",
    "            \"duration_sec\": round(duration_sec, 2),\n",
    "            \"sampling_rate\": sr,\n",
    "            \"dataset\": \"LibriSpeech\",\n",
    "            \"speaker_id\": speaker_id,\n",
    "            \"environment\": \"clean\",\n",
    "            \"text\": text\n",
    "        })\n",
    "\n",
    "    # 4. Save metadata\n",
    "    df = pd.DataFrame(rows)\n",
    "    df.to_csv(\"librispeech_metadata.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "04f5b208",
   "metadata": {},
   "outputs": [],
   "source": [
    "def unify_metadata(output_path, sources):\n",
    "    dfs = []\n",
    "    for src in sources:\n",
    "        if os.path.isfile(src):\n",
    "            print(f\"Found {src}, adding to unified dataset\")\n",
    "            df = pd.read_csv(src)\n",
    "\n",
    "            # Ensure required columns exist (fill missing if needed)\n",
    "            for col in [\"file\", \"duration_sec\", \"sampling_rate\", \"dataset\",\n",
    "                        \"speaker_id\", \"environment\", \"text\"]:\n",
    "                if col not in df.columns:\n",
    "                    df[col] = \"\"\n",
    "\n",
    "            # Make file paths relative\n",
    "            df[\"file\"] = df[\"file\"].apply(lambda x: os.path.relpath(x))\n",
    "            dfs.append(df)\n",
    "        else:\n",
    "            print(f\"{src} not found, skipping\")\n",
    "\n",
    "    if not dfs:\n",
    "        print(\"No metadata files found. Run dataset preprocessing first.\")\n",
    "        return\n",
    "\n",
    "    # Merge all datasets\n",
    "    full_df = pd.concat(dfs, ignore_index=True)\n",
    "\n",
    "    # Drop duplicates if any (based on file path)\n",
    "    full_df.drop_duplicates(subset=\"file\", inplace=True)\n",
    "\n",
    "    # Save unified metadata\n",
    "    full_df.to_csv(output_path, index=False)\n",
    "    print(f\"Unified metadata saved to {output_path}\")\n",
    "    print(f\"Total files: {len(full_df)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5f9f579d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File already exists.\n"
     ]
    }
   ],
   "source": [
    "file_path = \"metadata.csv\"\n",
    "\n",
    "# Check if the metadata already exists\n",
    "if os.path.isfile(file_path):\n",
    "    print(\"File already exists.\")\n",
    "else:\n",
    "    # List of dataset metadata files (expandable in the future)\n",
    "    sources = [\n",
    "        \"metadata_commonvoice.csv\",\n",
    "        \"librispeech_metadata.csv\",\n",
    "        # Add \"metadata_custom.csv\" later when they are ready]\n",
    "    ]\n",
    "    unify_metadata(\"metadata.csv\", sources)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
